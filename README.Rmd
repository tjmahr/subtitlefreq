---
output: 
  github_document:
    keep_html: false
    html_preview: false
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(tidyverse)
```

# subtitlefreq

<!-- badges: start -->
<!-- badges: end -->

The goal of subtitlefreq is to provide alternative word frequency counts from
the SUBTLEX-US corpus. My problem with the SUBTLEX-US corpus is that it
separates "word+t" into "word" + "t", so that the contractions "isn't",
"aren't", "don't", etc. count as two words. This project is a naive attempt 
to rebuild some frequency counts from scratch.

⚠️ This is not a drop-in replacement for the SUBTLEX-US corpus. I just wanted to
estimate how frequent "don't" is so I could compute some neighborhood frequency
measures.

## obtaining the raw SUBTLEX-US data

As a matter of caution, I won't provide the original subtitle corpus. But you
can download it the same way that I did.

- Go to the following URL: <http://www.lexique.org/?page_id=241> and download the
corpus. 
- Move `Subtlex US.rar` into `data` folder. 

We can test our download by reading the embedded readme file from it.

```{r}
readme <- archive::archive_read(
  "data/Subtlex US.rar", 
  "Subtlex US/readme.txt"
)

writeLines(readLines(readme))
```

That last line is why I don't redistribute the corpus.

## then run targets

Assuming the data are downloaded and the appropriate packages are
installed, then running `targets::tar_make()` should count the words in
the corpus. targets also [downloads](https://osf.io/djpqz/) and prepares the published version of
the SUBTLEX-US frequency counts.

```{r}
data_subtlexus <- targets::tar_read(data_subtlexus)
data_subtlexus
```




Data processing here works in three stages. First, there is a raw tibble of
lines with row per subtitle line. The `batch` column is used for splitting the
corpus into batches so that words can be counted in parallel.

```{r}
library(tidyverse)
data_raw <- targets::tar_read(data_raw_corpus)
data_raw
```

These lines are patched (to remove some garbage I discovered where markup was
included in the dialogue), and then the words are in batch are counted.

The counts from each batch are pooled together to give the following frequency
counts:

```{r}
data_pooled <- targets::tar_read(data_counts_pooled)
data_pooled |> print(n = 20)
```

You might notice that there are around 200,000 words here, instead of
the 60,000-70,000 words you might see elsewhere. This is probably because we 
did not lump any inflections together or anything.


If I see ever any typos in the counts, I try to store a correction in the csv
file `data/patches.csv`. This is probably a futile, never-ending endeavor, but 
hey, at least anyone can add to it.

```{r}
data_patches <- targets::tar_read("data_patches")
data_patches
```

These individual patches are followed by regular-expression-based
patches. These are stored in a `data/patches-regex.csv` so that they can
have a `comment` column that describes their behavior.

```{r}
data_patches_regex <- targets::tar_read("data_patches_regex")
data_patches_regex
```

After applying the patches, we obtain the following counts:

```{r}
data <- targets::tar_read("data_counts_patched")
data
```

We can rationalize our patching activity by looking at how many words are 
affected:

```{r}
data_pooled |>
  anti_join(data, by = "word") |> 
  print(n = 20)
```


## open questions (so far)

I'm not sure what's going on with the encoding and whether that matters. If you
find all lines with the string "Zellweger", you will find some `"Ren\xe9e
Zellweger"` and some `"Renee Zellweger"` tokens.

Numerals show up. I imagine that one would want to decompose them into subwords
so that, e.g., "1993" is "nineteen", "ninety", and "three".


```{r, echo = FALSE, eval = FALSE}
# sample(data$word, 20)
# data_raw <- targets::tar_read(data_raw_corpus_patched)
# x <- data$word |> str_subset("^i'")
# paste0(x,",",x)  |> writeLines()

```

## comparisons

What text-cleaning did they originally do?

> We rejected all files with more than 2.5% type errors according to the
> spelling checker Aspell. In the end, 8,388 films and television
> episodes were retained, with a total of 51.0 million words (16.1
> million from television series, 14.3 million from films before 1990,
> and 20.6 million from films after 1990).

I seem to be missing around 3 million tokens.

```{r}
sum(data$n)
51000000 - sum(data$n)
```

Or perhaps I am missing just 2 million words, based on the published
frequencies:

```{r}
sum(data_subtlexus$FREQcount)
sum(data_subtlexus$FREQcount) - sum(data$n)
```

Our raw text has lots of segmentation errors where multiple words are
combined together. For example, here are types with "in'" followed by 2
characters. If I split "tryin'to" into "tryin' to", I get 36 new words.
Would systematically fixing lots of segmentation mistakes
rediscover 2,000,000 missing words?

```{r}
data_pooled |> 
  filter(str_detect(word, "in'..+$")) |> 
  mutate(sum = sum(n))
```


> For instance, there are 18,081 occurrences of the word *play* in
> SUBTLEXUS, 1,521 of the word *plays*, 2,870 of the word *played*,
> and 7,515 of the word *playing*.

```{r}
data |> 
  filter(word %in% c("playing", "plays", "play", "played")) 
```

This is a pretty good match, but we have a few more *playing* tokens
because we patched `"lng"` words.

The published counts fortunately do not have "lng" words.

```{r}
data_subtlexus |> 
  filter(str_detect(Word, "lng"))
```

But the combination of treating contractions as separate words and the
l-\>i conversion means that there are a few thousand spurious tokens of
"il":

```{r}
# raw, unpatched
data_pooled |> 
  filter(str_detect(word, "^il$|'il")) |> 
  mutate(sum(n))

# published
data_subtlexus |> 
  filter(str_detect(Word, "^il$")) |> 
  select(1:2)
```

Because of OCR converting "I" to "l", I patched the corpus to replace "l"
with "I" when it was inside of an all-caps word. We can see the big
differences in the counts between my counts and the published ones for
certain initialisms.

```{r}
# raw, unpatched
data_pooled |> 
  filter(word %in% c("fbi", "irs", "cia")) |> 
  arrange(word)

# published
data_subtlexus |> 
  filter(tolower(Word) %in% c("fbi", "irs", "cia")) |> 
  arrange(Word) |> 
  select(1:2)
```



## let's try something

I saw the textstem package as a solution for lemmatizing words. 

```{r}
data |> 
  head(1000) |> 
  mutate(
    lemma = textstem::lemmatize_words(word)
  ) |> 
  group_by(lemma) |> 
  mutate(lemma_n = sum(n)) |> 
  arrange(desc(lemma_n)) |> 
  print(n = 20)
```

Hmm, I wish I could skip irregular forms from being lemmatized. I am
also not a fan of "being" is reduced down to "be".
